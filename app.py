import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import time
import random
import json
from urllib.parse import quote, urlencode, urlparse, parse_qs
from PIL import Image
from io import BytesIO

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="YouTube Smart Scraper",
    page_icon="üß†",
    layout="wide"
)

# CSS customizado
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    color: #FF0000;
    text-align: center;
    margin-bottom: 1rem;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
}

.smart-info {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 20px;
    border-radius: 15px;
    margin-bottom: 20px;
}

.video-card {
    border: 1px solid #ddd;
    border-radius: 12px;
    padding: 15px;
    margin: 15px 0;
    background: linear-gradient(135deg, #f9f9f9 0%, #ffffff 100%);
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    transition: transform 0.2s;
}

.video-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 12px rgba(0,0,0,0.15);
}

.video-title {
    font-size: 1.3rem;
    font-weight: bold;
    color: #333;
    margin-bottom: 8px;
    line-height: 1.3;
}

.viral-badge {
    background: linear-gradient(45deg, #FFD700, #FFA500);
    color: #333;
    padding: 4px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    font-weight: bold;
    margin-left: 5px;
}

.recent-badge {
    background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
    color: white;
    padding: 4px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    font-weight: bold;
    margin-left: 5px;
}

.metric-container {
    background: #f0f2f6;
    padding: 15px;
    border-radius: 10px;
    text-align: center;
    margin: 10px 0;
}

.strategy-box {
    background: #e8f5e8;
    color: #2d5a3d;
    padding: 15px;
    border-radius: 8px;
    border-left: 4px solid #28a745;
    margin: 20px 0;
}
</style>
""", unsafe_allow_html=True)

class YouTubeSmartScraper:
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """Configurar sess√£o com headers ultra-realistas"""
        # Headers que imitam navegador real
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        })
    
    def human_delay(self, min_sec=1, max_sec=3):
        """Delay humano aleat√≥rio"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    def extract_views_number(self, text):
        """Extrair n√∫mero de views de texto"""
        if not text:
            return 0
        
        text = text.lower().replace('.', '').replace(',', '').replace(' ', '')
        
        multipliers = {
            'mil': 1000, 'k': 1000,
            'milh√£o': 1000000, 'milh√µes': 1000000, 'm': 1000000, 'mi': 1000000,
            'bilh√£o': 1000000000, 'bilh√µes': 1000000000, 'b': 1000000000
        }
        
        for suffix, multiplier in multipliers.items():
            if suffix in text:
                numbers = re.findall(r'\d+(?:\.\d+)?', text.replace(suffix, ''))
                if numbers:
                    return int(float(numbers[0]) * multiplier)
        
        numbers = re.findall(r'\d+', text)
        return int(numbers[0]) if numbers else 0
    
    def get_trending_videos(self, max_videos=50):
        """Obter v√≠deos trending usando m√∫ltiplas estrat√©gias"""
        videos = []
        
        # Estrat√©gia 1: RSS Feeds do YouTube (funciona sempre!)
        try:
            st.info("üì° Acessando RSS feeds do YouTube...")
            
            # RSS feeds por categoria
            rss_urls = [
                'https://www.youtube.com/feeds/videos.xml?chart=most_popular',  # Mais populares
            ]
            
            for rss_url in rss_urls:
                try:
                    response = self.session.get(rss_url, timeout=10)
                    if response.status_code == 200:
                        # Parse XML simples
                        entries = re.findall(r'<entry>(.*?)</entry>', response.text, re.DOTALL)
                        
                        for entry in entries[:20]:  # Limitar para n√£o sobrecarregar
                            video_data = self.parse_rss_entry(entry)
                            if video_data:
                                videos.append(video_data)
                except:
                    continue
                
                self.human_delay(0.5, 1)
        except Exception as e:
            st.warning(f"RSS feeds n√£o dispon√≠veis: {str(e)}")
        
        # Estrat√©gia 2: Scraping inteligente da p√°gina trending
        try:
            st.info("üéØ Scraping inteligente da p√°gina trending...")
            trending_videos = self.scrape_trending_smart()
            videos.extend(trending_videos)
        except Exception as e:
            st.warning(f"Scraping trending falhou: {str(e)}")
        
        # Estrat√©gia 3: Usar buscas por termos virais
        try:
            st.info("üî• Buscando por termos virais...")
            viral_terms = ['viral', 'bomba', 'explos√£o', 'trending', 'hot']
            
            for term in viral_terms[:2]:  # Limitar para n√£o sobrecarregar
                search_videos = self.search_videos_smart(term, max_results=10)
                videos.extend(search_videos)
                self.human_delay(1, 2)
        except Exception as e:
            st.warning(f"Busca viral falhou: {str(e)}")
        
        # Remover duplicatas e limitar
        unique_videos = {}
        for video in videos:
            video_id = video.get('id')
            if video_id and video_id not in unique_videos:
                unique_videos[video_id] = video
        
        final_videos = list(unique_videos.values())[:max_videos]
        
        # Enriquecer dados com informa√ß√µes adicionais
        for video in final_videos:
            try:
                self.enrich_video_data(video)
            except:
                continue
        
        return final_videos
    
    def parse_rss_entry(self, entry):
        """Parse de entrada RSS"""
        try:
            # Extrair dados b√°sicos do XML
            video_id_match = re.search(r'yt:videoId>([^<]+)', entry)
            title_match = re.search(r'<title>([^<]+)</title>', entry)
            channel_match = re.search(r'<name>([^<]+)</name>', entry)
            published_match = re.search(r'<published>([^<]+)</published>', entry)
            
            if video_id_match and title_match:
                return {
                    'id': video_id_match.group(1),
                    'title': title_match.group(1),
                    'channel_name': channel_match.group(1) if channel_match else 'Desconhecido',
                    'published_time': published_match.group(1) if published_match else '',
                    'url': f'https://www.youtube.com/watch?v={video_id_match.group(1)}',
                    'source': 'RSS'
                }
        except:
            pass
        return None
    
    def scrape_trending_smart(self):
        """Scraping inteligente da p√°gina trending"""
        videos = []
        try:
            # Acesso direto √† p√°gina trending
            url = 'https://www.youtube.com/feed/trending'
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                # Extrair dados usando regex inteligente
                video_patterns = [
                    r'{"videoId":"([^"]+)".*?"title":{"runs":\[{"text":"([^"]+)"}\].*?"ownerText":{"runs":\[{"text":"([^"]+)"',
                    r'"watchEndpoint":{"videoId":"([^"]+)".*?"text":"([^"]+)".*?"shortBylineText":{"runs":\[{"text":"([^"]+)"'
                ]
                
                for pattern in video_patterns:
                    matches = re.findall(pattern, response.text)
                    for match in matches:
                        if len(match) >= 3:
                            video_data = {
                                'id': match[0],
                                'title': match[1],
                                'channel_name': match[2],
                                'url': f'https://www.youtube.com/watch?v={match[0]}',
                                'source': 'Trending'
                            }
                            videos.append(video_data)
                
                # Limitar e remover duplicatas
                unique_ids = set()
                unique_videos = []
                for video in videos:
                    if video['id'] not in unique_ids:
                        unique_ids.add(video['id'])
                        unique_videos.append(video)
                
                return unique_videos[:20]
        
        except Exception as e:
            pass
        
        return videos
    
    def search_videos_smart(self, query, max_results=20):
        """Busca inteligente no YouTube"""
        videos = []
        try:
            search_url = f'https://www.youtube.com/results?search_query={quote(query)}'
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                # Padr√µes para extrair dados de v√≠deos
                patterns = [
                    r'"videoId":"([^"]+)".*?"title":{"accessibility":{"accessibilityData":{"label":"([^"]+)"}}.*?"longBylineText":{"runs":\[{"text":"([^"]+)"',
                    r'"watchEndpoint":{"videoId":"([^"]+)".*?"text":"([^"]+)".*?"ownerText":{"runs":\[{"text":"([^"]+)"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, response.text)
                    for match in matches:
                        if len(match) >= 3:
                            video_data = {
                                'id': match[0],
                                'title': match[1],
                                'channel_name': match[2],
                                'url': f'https://www.youtube.com/watch?v={match[0]}',
                                'source': f'Search: {query}'
                            }
                            videos.append(video_data)
                
                return videos[:max_results]
        
        except Exception as e:
            pass
        
        return videos
    
    def enrich_video_data(self, video):
        """Enriquecer dados do v√≠deo com informa√ß√µes adicionais"""
        try:
            # Acessar p√°gina do v√≠deo para obter mais dados
            video_url = video['url']
            response = self.session.get(video_url, timeout=10)
            
            if response.status_code == 200:
                html = response.text
                
                # Extrair views
                view_patterns = [
                    r'"viewCount":"(\d+)"',
                    r'(\d+(?:\.\d+)?[KMB]?) visualiza√ß√µes',
                    r'(\d+(?:,\d+)*) views'
                ]
                
                for pattern in view_patterns:
                    match = re.search(pattern, html, re.IGNORECASE)
                    if match:
                        views_text = match.group(1)
                        video['views_text'] = views_text
                        video['views'] = self.extract_views_number(views_text)
                        break
                
                # Extrair data de publica√ß√£o
                pub_patterns = [
                    r'"publishDate":"([^"]+)"',
                    r'h√° (\d+) (hora|horas|dia|dias|semana|semanas|m√™s|meses)',
                    r'(\d+) (hora|horas|dia|dias) atr√°s'
                ]
                
                for pattern in pub_patterns:
                    match = re.search(pattern, html, re.IGNORECASE)
                    if match:
                        video['published_time'] = match.group(0)
                        break
                
                # Extrair thumbnail
                thumb_match = re.search(r'"videoDetails":{"videoId":"[^"]+","title":"[^"]+","lengthSeconds":"[^"]+","keywords":\[[^\]]*\],"channelId":"[^"]+","isLiveContent":[^,]+,"shortDescription":"[^"]*","isCrawlable":[^,]+,"thumbnail":{"thumbnails":\[{"url":"([^"]+)"', html)
                if thumb_match:
                    video['thumbnail'] = thumb_match.group(1)
                
        except Exception as e:
            pass

def format_number(num):
    """Formatar n√∫meros grandes"""
    if num >= 1000000000:
        return f"{num/1000000000:.1f}B"
    elif num >= 1000000:
        return f"{num/1000000:.1f}M"
    elif num >= 1000:
        return f"{num/1000:.1f}K"
    return str(num)

def is_recent_video(published_time):
    """Verificar se √© v√≠deo recente"""
    if not published_time:
        return False
    recent_keywords = ['hora', 'horas', 'minuto', 'minutos', '1 dia', '2 dias', 'h√° 1', 'h√° 2']
    return any(keyword in published_time.lower() for keyword in recent_keywords)

def is_viral_candidate(video):
    """Verificar se √© candidato viral"""
    views = video.get('views', 0)
    published = video.get('published_time', '')
    
    return (
        views > 50000 and 
        is_recent_video(published)
    ) or views > 500000  # Ou muitas views independente da data

def calculate_viral_score(video):
    """Calcular score viral"""
    views = video.get('views', 0)
    is_recent = is_recent_video(video.get('published_time', ''))
    
    score = views
    if is_recent:
        score *= 2  # Bonus para v√≠deos recentes
    
    return score

def display_smart_video_card(video):
    """Exibir card do v√≠deo"""
    col1, col2 = st.columns([1, 2.5])
    
    with col1:
        # Thumbnail
        if video.get('thumbnail'):
            try:
                st.image(video['thumbnail'], width=200)
            except:
                # Thumbnail padr√£o baseado no ID
                thumbnail_url = f"https://img.youtube.com/vi/{video.get('id', '')}/hqdefault.jpg"
                st.image(thumbnail_url, width=200)
        else:
            thumbnail_url = f"https://img.youtube.com/vi/{video.get('id', '')}/hqdefault.jpg"
            st.image(thumbnail_url, width=200)
    
    with col2:
        # Badges
        badges_html = ""
        if is_viral_candidate(video):
            badges_html += '<span class="viral-badge">üíé VIRAL</span>'
        if is_recent_video(video.get('published_time', '')):
            badges_html += '<span class="recent-badge">‚ö° RECENTE</span>'
        
        # Score viral
        viral_score = calculate_viral_score(video)
        
        st.markdown(f"""
        <div class="video-card">
            <div class="video-title">{video.get('title', 'Sem t√≠tulo')} {badges_html}</div>
            
            <div style="color: #666; font-size: 1rem; margin-bottom: 8px;">
                üì∫ <strong>{video.get('channel_name', 'Canal desconhecido')}</strong>
                <span style="color: #999; margin-left: 10px;">({video.get('source', 'Desconhecido')})</span>
            </div>
            
            <div style="color: #0066cc; font-size: 0.9rem; font-weight: 600; margin-bottom: 8px;">
                üëÅÔ∏è {video.get('views_text', 'N/A')} visualiza√ß√µes ‚Ä¢ 
                üìÖ {video.get('published_time', 'Data N/A')} ‚Ä¢
                üî• Score: {format_number(viral_score)}
            </div>
            
            <div style="margin-top: 15px;">
                <a href="{video.get('url', '#')}" target="_blank" style="color: #FF0000; text-decoration: none; margin-right: 20px; font-weight: bold;">
                    üé• ASSISTIR V√çDEO
                </a>
                <a href="{video.get('channel_url', '#')}" target="_blank" style="color: #0066cc; text-decoration: none; font-weight: bold;">
                    üì∫ VER CANAL
                </a>
            </div>
        </div>
        """, unsafe_allow_html=True)

# Interface principal
st.markdown('<h1 class="main-header">üß† YouTube Smart Scraper</h1>', unsafe_allow_html=True)

# Info sobre Smart Scraper
st.markdown("""
<div class="smart-info">
    <h3>üß† Smart Scraper - M√∫ltiplas Estrat√©gias Inteligentes!</h3>
    <ul style="margin: 10px 0;">
        <li><strong>üì° RSS Feeds</strong> - dados oficiais sempre funcionam</li>
        <li><strong>üéØ Scraping inteligente</strong> - m√∫ltiplos padr√µes de extra√ß√£o</li>
        <li><strong>üî• Busca viral</strong> - termos que identificam conte√∫do em alta</li>
        <li><strong>üß† Enriquecimento de dados</strong> - extrai views, data e mais info</li>
    </ul>
</div>
""", unsafe_allow_html=True)

# Estrat√©gia
st.markdown("""
<div class="strategy-box">
    <h4>üí° Estrat√©gia Multi-Fonte:</h4>
    <p>Este scraper usa <strong>3 estrat√©gias simultaneamente</strong> para garantir resultados mesmo se uma falhar:</p>
    <p><strong>1. RSS Feeds</strong> ‚Üí <strong>2. Scraping Trending</strong> ‚Üí <strong>3. Busca por Termos Virais</strong></p>
</div>
""", unsafe_allow_html=True)

# Controles
col1, col2 = st.columns(2)

with col1:
    max_videos = st.selectbox("üìä M√°ximo de v√≠deos:", [20, 30, 50, 75], index=1)

with col2:
    min_views = st.selectbox("üëÅÔ∏è Views m√≠nimas:", [0, 10000, 50000, 100000, 500000], index=1)

# Bot√£o principal
if st.button("üß† EXECUTAR SMART SCRAPING!", type="primary", use_container_width=True):
    
    # Inicializar scraper
    scraper = YouTubeSmartScraper()
    
    with st.spinner("üß† Executando estrat√©gias inteligentes..."):
        videos = scraper.get_trending_videos(max_videos)
        
        # Filtrar por views m√≠nimas
        if min_views > 0:
            videos = [v for v in videos if v.get('views', 0) >= min_views]
        
        if videos:
            # Ordenar por score viral
            videos.sort(key=calculate_viral_score, reverse=True)
            
            # Estat√≠sticas
            viral_count = sum(1 for v in videos if is_viral_candidate(v))
            recent_count = sum(1 for v in videos if is_recent_video(v.get('published_time', '')))
            total_views = sum(v.get('views', 0) for v in videos)
            avg_score = sum(calculate_viral_score(v) for v in videos) / len(videos) if videos else 0
            
            # M√©tricas
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.markdown(f"""
                <div class="metric-container">
                    <h3 style="margin: 0; color: #333;">üéØ {len(videos)}</h3>
                    <p style="margin: 0; color: #666;">V√≠deos Encontrados</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="metric-container">
                    <h3 style="margin: 0; color: #FFD700;">üíé {viral_count}</h3>
                    <p style="margin: 0; color: #666;">Candidatos Virais</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col3:
                st.markdown(f"""
                <div class="metric-container">
                    <h3 style="margin: 0; color: #FF6B6B;">‚ö° {recent_count}</h3>
                    <p style="margin: 0; color: #666;">V√≠deos Recentes</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col4:
                st.markdown(f"""
                <div class="metric-container">
                    <h3 style="margin: 0; color: #4ECDC4;">üî• {format_number(avg_score)}</h3>
                    <p style="margin: 0; color: #666;">Score Viral M√©dio</p>
                </div>
                """, unsafe_allow_html=True)
            
            # Export
            if st.button("üì• Exportar Resultados Smart", key="export_smart"):
                df = pd.DataFrame(videos)
                csv = df.to_csv(index=False)
                st.download_button(
                    label="‚¨áÔ∏è Download CSV Smart",
                    data=csv,
                    file_name=f"smart_scraping_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    key="download_smart"
                )
            
            # Resultados
            st.markdown("---")
            st.markdown("## üß† Resultados Smart Scraping:")
            st.markdown("*Ordenado por Score Viral (views √ó bonus rec√™ncia)*")
            
            for video in videos:
                display_smart_video_card(video)
                st.markdown("---")
        
        else:
            st.warning("‚ùå Nenhum v√≠deo encontrado com os crit√©rios selecionados")

# Instru√ß√µes
st.markdown("""
## üöÄ Como funciona:

### üì° **RSS Feeds (sempre funciona):**
- Acessa feeds oficiais do YouTube
- Dados sempre atualizados
- Imposs√≠vel de bloquear

### üéØ **Scraping Inteligente:**
- M√∫ltiplos padr√µes de extra√ß√£o
- Fallbacks autom√°ticos
- M√°xima compatibilidade

### üî• **Busca Viral:**
- Termos que identificam conte√∫do em alta
- Captura tend√™ncias emergentes
- Score viral calculado automaticamente

**Resultado: Muito mais v√≠deos que qualquer API limitada!** üéØ
""")

# Rodap√©
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 20px;">
    <p>üß† <strong>YouTube Smart Scraper</strong> | Intelig√™ncia artificial aplicada</p>
    <p>M√∫ltiplas estrat√©gias para m√°xima efic√°cia!</p>
</div>
""", unsafe_allow_html=True)
